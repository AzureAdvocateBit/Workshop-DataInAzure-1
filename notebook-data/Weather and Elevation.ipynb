{
  "cells": [
    {
      "source": [
        "# Introduction to Data in Azure \n",
        "\n",
        "This notebook uses Surface level data from NOAA to explore how elevation affects weather conditions.\n",
        "\n",
        "\n",
        "[Be sure to follow the instructions in the repository](https://github.com/paladique/Workshop-DataInAzure/blob/master/README.md) before continuing with this lab. \n",
        "\n",
        "First we'll install some packages, grab the [data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Data Sets."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install packages\n",
        "import os, sys\n",
        "\n",
        "!{sys.executable} -m pip install azure-storage-blob\n",
        "!{sys.executable} -m pip install pyarrow\n",
        "!{sys.executable} -m pip install pandas"
      ],
      "metadata": {
        "scrolled": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: azure-storage-blob in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (12.5.0)\nRequirement already satisfied: msrest>=0.6.10 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from azure-storage-blob) (0.6.19)\nRequirement already satisfied: cryptography>=2.1.4 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from azure-storage-blob) (3.1)\nRequirement already satisfied: azure-core<2.0.0,>=1.6.0 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from azure-storage-blob) (1.8.1)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from msrest>=0.6.10->azure-storage-blob) (1.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from msrest>=0.6.10->azure-storage-blob) (2020.6.20)\nRequirement already satisfied: isodate>=0.6.0 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from msrest>=0.6.10->azure-storage-blob) (0.6.0)\nRequirement already satisfied: requests~=2.16 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from msrest>=0.6.10->azure-storage-blob) (2.24.0)\nRequirement already satisfied: six>=1.4.1 in c:\\users\\jasmineg\\appdata\\roaming\\python\\python38\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.0)\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.2)\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-storage-blob) (3.1.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests~=2.16->msrest>=0.6.10->azure-storage-blob) (1.25.10)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests~=2.16->msrest>=0.6.10->azure-storage-blob) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests~=2.16->msrest>=0.6.10->azure-storage-blob) (2.10)\nRequirement already satisfied: pycparser in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob) (2.20)\nRequirement already satisfied: pyarrow in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.0.1)\nRequirement already satisfied: numpy>=1.14 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyarrow) (1.19.2)\nRequirement already satisfied: pandas in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.1.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jasmineg\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.8.1)\nRequirement already satisfied: numpy>=1.15.4 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.19.2)\nRequirement already satisfied: pytz>=2017.2 in c:\\users\\jasmineg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2020.1)\nRequirement already satisfied: six>=1.5 in c:\\users\\jasmineg\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure storage access info from open data set\n",
        "azure_storage_account_name = \"azureopendatastorage\"\n",
        "azure_storage_sas_token = r\"\"\n",
        "container_name = \"isdweatherdatacontainer\"\n",
        "folder_name = \"ISDWeather/\""
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "\n",
        "if azure_storage_account_name is None or azure_storage_sas_token is None:\n",
        "    raise Exception(\n",
        "        \"Provide your specific name and key for your Azure Storage account--see the Prerequisites section earlier.\")\n",
        "\n",
        "print('Looking for the first parquet under the folder ' +\n",
        "      folder_name + ' in container \"' + container_name + '\"...')\n",
        "container_url = f\"https://{azure_storage_account_name}.blob.core.windows.net/\"\n",
        "blob_service_client = BlobServiceClient(\n",
        "    container_url, azure_storage_sas_token if azure_storage_sas_token else None)\n",
        "\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "blobs = container_client.list_blobs(folder_name)\n",
        "sorted_blobs = sorted(list(blobs), key=lambda e: e.name, reverse=True)\n",
        "targetBlobName = ''\n",
        "for blob in sorted_blobs:\n",
        "    if blob.name.startswith(folder_name) and blob.name.endswith('.parquet'):\n",
        "        targetBlobName = blob.name\n",
        "        break\n",
        "\n",
        "print('Target blob to download: ' + targetBlobName)\n",
        "_, filename = os.path.split(targetBlobName)\n",
        "blob_client = container_client.get_blob_client(targetBlobName)\n",
        "with open(filename, 'wb') as local_file:\n",
        "    blob_client.download_blob().download_to_stream(local_file)"
      ],
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking for the first parquet under the folder ISDWeather/ in container \"isdweatherdatacontainer\"...\nTarget blob to download: ISDWeather/year=2020/month=9/part-00007-tid-8228874701277795085-7b8726ea-d43e-4602-80de-455ce4f066a5-2406-9.c000.snappy.parquet\n"
        }
      ],
      "execution_count": 14
    },
    {
      "source": [
        "# Querying the data\n",
        "\n",
        "A Parquet file was downloaded with recent weather data. \n",
        "\n",
        "The following query will compare the weather of two areas in Colorado that differ in elevation.\n",
        "\n",
        "![](colorado.png)\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the parquet file into Pandas data frame\n",
        "import pandas as pd\n",
        "\n",
        "print('Reading the parquet file into Pandas data frame')\n",
        "df = pd.read_parquet(filename, columns=['datetime', 'latitude', 'longitude', 'elevation', 'stationName', 'temperature', 'windSpeed'])\n",
        "\n",
        "df.query('(38.503 <= latitude <= 39.887) & (-108.294 <= longitude <= -105.943)')\n",
        "df.query('(39.375.503 <= latitude <= 39.887) & (-108.294 <= longitude <= -105.943)')"
      ],
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Reading the parquet file into Pandas data frame\n"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filename' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-2a5e75e996af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading the parquet file into Pandas data frame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'longitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'elevation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stationName'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'temperature'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'windSpeed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(38.503 <= latitude <= 39.887) & (-108.294 <= longitude <= -105.943)'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'filename' is not defined"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "source": [
        "## Using Azure Data Factory\n",
        "\n",
        "We will convert this semi-structed data into relational data with Azure Data Factory.\n",
        "\n",
        "\n",
        "1. The previous cells should have produced a Parquet file, and you should see it in the file directory. If not, you can use the `weather.parquet` file. \n",
        "2. Download the created parquet file\n",
        "3. In Azure ML Studio, navigate to the menu on the left, open the **Datastores** option (database icon, under **Manage**)\n",
        "4. Open/Click the workspaceblob(Default) datatore \n",
        "5. Click the storage account name under **Account Name**\n",
        "6. In the new tab you will see the storage account overview. Go to **Containers**\n",
        "7. Select the container that starts with `azureml-` and click the upload icon and select the Parquet file.\n",
        "\n",
        "\n",
        "1. From the Azure Portal, Open your Data Factory and select **Author and Monitor**, which will open a new tab.\n",
        "2. In the Data Factory home page, select **Copy Data** to setup the manual task \n",
        "3. After clicking Next on Properties, let's create our data connections. \n",
        "\n",
        "    3a. Select **+ Create New Connection**\n",
        "    \n",
        "    3b. Search for Blob Storage, select **Azure Blob Storage** > **Next**\n",
        "    \n",
        "    3c. Select your Azure Subscirption and your Storage Account Name, select **Create**\n",
        "    \n",
        "    3d. Repeat this process for Azure SQL Database and use SQL authentication \n",
        "    \n",
        "    Optional: Test your connection\n",
        "\n",
        "4. Select Azure Blob Storage connection as the source > **Next** \n",
        "5. Select **Browse** on the right hand side, select your container and click **Choose** on `bing_covid-19_data.parquet` > **Next** \n",
        "6. Confirm the file format is json, select it if not and click **Next**\n",
        "4. Select Azure SQL connection as the destination target > **Next** \n",
        "4. Select the CovidData databsase as the destination target > **Next** \n",
        "5. We're only interested in the `id, updated, and confirmed` columns, deleting the other rows is optional > click **Next** until you reach the `Deployment complete` window\n",
        "7. Loading this data will take a few minutes.\n",
        "\n",
        "### To much clicking? \n",
        "You can build data piplines in Data Factory with the command line"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Querying from the Database\n",
        "\n",
        "Now that the data is in Azure SQL, lets query it. Be sure to add and update `myconfig.cfg` with the following:\n",
        "\n",
        "  ```python\n",
        "[my_db]\n",
        "server: [your Azure SQL server name]\n",
        "database: [your Azure SQL database name]\n",
        "username: [your Azure SQL username]\n",
        "password: [your Azure SQL password]\n",
        "  ```\n",
        "  \n",
        "  **Note: The file will contain critical information. Avoid setting your notebook public until they are removed.**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from configparser import ConfigParser\n",
        "parser = ConfigParser()\n",
        "_ = parser.read('myconfig.cfg')\n",
        "\n",
        "\n",
        "server = parser.get('my_db', 'server')\n",
        "database = parser.get('my_db', 'database')\n",
        "username = parser.get('my_db', 'username')\n",
        "password = parser.get('my_db', 'password')\n",
        "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"SELECT * FROM [dbo].[CovidData] WHERE country_region='United States'\" \n",
        "df_covid_us_sql = pd.read_sql(query, cnxn)\n",
        "df_covid_us_sql['country_region'] = f['country_region'].str.strip()\n",
        "df_covid_us_sql.head(10)\n",
        "\n",
        "df_covid_us_sql[['confirmed','updated']].groupby(pd.Grouper(key='updated',freq='W')).sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "language": "python",
      "name": "python_defaultSpec_1600270877652"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.5-final",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "LoadAzureBlobParquet",
    "notebookId": 576599826755691
  },
  "nbformat": 4,
  "nbformat_minor": 0
}