{
  "cells": [
    {
      "source": [
        "# Introduction to Data in Azure \n",
        "\n",
        "This notebook uses Surface level data from NOAA to explore how elevation affects weather conditions.\n",
        "\n",
        "\n",
        "[Be sure to follow the instructions in the repository](https://github.com/paladique/Workshop-DataInAzure/blob/master/README.md) before continuing with this lab. \n",
        "\n",
        "First we'll install some packages, grab the [data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Data Sets."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install packages\n",
        "import os, sys\n",
        "\n",
        "!{sys.executable} -m pip install azure-storage-blob\n",
        "!{sys.executable} -m pip install pyarrow\n",
        "!{sys.executable} -m pip install pandas\n",
        "!{sys.executable} -m pip install pymysql\n",
        "!{sys.executable} -m pip install sqlalchemy"
      ],
      "metadata": {
        "scrolled": false,
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure storage access info from open data set\n",
        "azure_storage_account_name = \"azureopendatastorage\"\n",
        "azure_storage_sas_token = r\"\"\n",
        "container_name = \"isdweatherdatacontainer\"\n",
        "folder_name = \"ISDWeather/\""
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "\n",
        "if azure_storage_account_name is None or azure_storage_sas_token is None:\n",
        "    raise Exception(\n",
        "        \"Provide your specific name and key for your Azure Storage account--see the Prerequisites section earlier.\")\n",
        "\n",
        "print('Looking for the first parquet under the folder ' +\n",
        "      folder_name + ' in container \"' + container_name + '\"...')\n",
        "container_url = f\"https://{azure_storage_account_name}.blob.core.windows.net/\"\n",
        "blob_service_client = BlobServiceClient(\n",
        "    container_url, azure_storage_sas_token if azure_storage_sas_token else None)\n",
        "\n",
        "container_client = blob_service_client.get_container_client(container_name)\n",
        "blobs = container_client.list_blobs(folder_name)\n",
        "sorted_blobs = sorted(list(blobs), key=lambda e: e.name, reverse=True)\n",
        "targetBlobName = ''\n",
        "for blob in sorted_blobs:\n",
        "    if blob.name.startswith(folder_name) and blob.name.endswith('.parquet'):\n",
        "        targetBlobName = blob.name\n",
        "        break\n",
        "\n",
        "print('Target blob to download: ' + targetBlobName)\n",
        "_, filename = os.path.split(targetBlobName)\n",
        "blob_client = container_client.get_blob_client(targetBlobName)\n",
        "with open(filename, 'wb') as local_file:\n",
        "    blob_client.download_blob().download_to_stream(local_file)"
      ],
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# Querying the data\n",
        "\n",
        "A Parquet file was downloaded with recent weather data. \n",
        "\n",
        "The following query will compare the weather of two areas in Colorado that differ in elevation.\n",
        "\n",
        "![](colorado.png)\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the parquet file into Pandas data frame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print('Reading the parquet file into Pandas data frame')\n",
        "df = pd.read_parquet(filename, columns=['datetime', 'latitude', 'longitude', 'elevation', 'stationName', 'temperature', 'windSpeed'])\n",
        "\n",
        "s1 = df.query('(38.503 <= latitude <= 39.887) & (-108.294 <= longitude <= -105.943)')\n",
        "s2 = df.query('(39.375 <= latitude <= 40.139) & (-104.943 <= longitude <= -102.186)')\n",
        "\n",
        "\n",
        "grouped = pd.concat([s1, s2], axis=0).groupby('stationName')\n",
        "grouped['temperature', 'windSpeed', 'elevation'].agg(np.mean)                                               "
      ],
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "## Using Azure Data Factory\n",
        "\n",
        "We will convert this semi-structed data into relational data with Azure Data Factory.\n",
        "\n",
        "\n",
        "1. The previous cells should have produced a Parquet file, and you should see it in the file directory. If not, you can use the `weather.parquet` file. \n",
        "2. Download the created parquet file\n",
        "3. In Azure ML Studio, navigate to the menu on the left, open the **Datastores** option (database icon, under **Manage**)\n",
        "4. Open/Click the workspaceblob(Default) datatore \n",
        "5. Click the storage account name under **Account Name**\n",
        "6. In the new tab you will see the storage account overview. Go to **Containers**\n",
        "7. Select the container that starts with `azureml-` and click the upload icon and select the Parquet file.\n",
        "\n",
        "\n",
        "1. From the Azure Portal, Open your Data Factory and select **Author and Monitor**, which will open a new tab.\n",
        "2. In the Data Factory home page, select **Copy Data** to setup the manual task \n",
        "3. After clicking Next on Properties, let's create our data connections. \n",
        "\n",
        "    3a. Select **+ Create New Connection**\n",
        "    \n",
        "    3b. Search for Blob Storage, select **Azure Blob Storage** > **Next**\n",
        "    \n",
        "    3c. Select your Azure Subscirption and your Storage Account Name, select **Create**\n",
        "    \n",
        "    3d. Repeat this process for Azure SQL Database and use SQL authentication \n",
        "    \n",
        "    Optional: Test your connection\n",
        "\n",
        "4. Select Azure Blob Storage connection as the source > **Next** \n",
        "5. Select **Browse** on the right hand side, select your container and click **Choose** on `[filename].snappy.parquet` > **Next** \n",
        "6. Confirm the file format is json, select it if not and click **Next**\n",
        "4. Select MySQL connection as the destination target > **Next** \n",
        "4. Select the `weather` databsase as the destination target > **Next** \n",
        "5. We're only interested in the `id, updated, and confirmed` columns, deleting the other rows is optional > click **Next** until you reach the `Deployment complete` window\n",
        "7. Loading this data will take a few minutes.\n",
        "\n",
        "### To much clicking? \n",
        "You can build data piplines in Data Factory with the command line"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Querying from the Database\n",
        "\n",
        "Now that the data is in MySQL, lets query it. Be sure to add and update `myconfig.cfg` with the following:\n",
        "\n",
        "  ```python\n",
        "[my_db]\n",
        "host: [your Azure SQL server name]\n",
        "database: [your Azure SQL database name]\n",
        "user: [your Azure SQL username]\n",
        "password: [your Azure SQL password]\n",
        "  ```\n",
        "  \n",
        "  **Note: The file will contain critical information. Avoid setting your notebook public until they are removed.**"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymysql\n",
        "import requests\n",
        "from configparser import ConfigParser\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "## By default, MySQL requires a SSL certificate, this is the defaut certificate provided by Microsoft. The following few lines downloads the cert and places it in the root directory. Visit this link for more info: https://docs.microsoft.com/en-us/azure/mysql/howto-configure-ssl\n",
        " \n",
        "url = 'https://cacerts.digicert.com/BaltimoreCyberTrustRoot.crt.pem'\n",
        "certificate = requests.get(url)\n",
        "\n",
        "with open('cert.crt.pem', 'wb') as r: \n",
        "    r.write(certificate.content)\n",
        "\n",
        "parser = ConfigParser()\n",
        "_ = parser.read('myconfig.cfg')\n",
        "\n",
        "\n",
        "host = parser.get('my_db', 'host')\n",
        "database = parser.get('my_db', 'database')\n",
        "user = parser.get('my_db', 'user')\n",
        "password = parser.get('my_db', 'password')\n",
        "\n",
        "# db_connection_str = 'mysql+pymysql://mysql_user:mysql_password@mysql_host/mysql_db'\n",
        "\n",
        "db_connection_str = 'mysql+pymysql://{}:{}@{}/{}?&ssl_ca=cert.crt.pem'\n",
        "\n",
        "db_connection = create_engine(db_connection_str.format(user, password, host, database))\n",
        "\n",
        "sqlDf = pd.read_sql('SELECT * FROM surfaceLevelWeather', con=db_connection)\n",
        "sqlSeries1 = sqlDf.query('(38.503 <= latitude <= 39.887) & (-108.294 <= longitude <= -105.943)')\n",
        "sqlSeries2 = sqlDf.query('(39.375 <= latitude <= 40.139) & (-104.943 <= longitude <= -102.186)')\n",
        "\n",
        "\n",
        "grouped = pd.concat([sqlSeries1, sqlSeries2], axis=0).groupby('station')\n",
        "grouped['temperature', 'windSpeed', 'elevation'].agg(np.mean)                                               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python_defaultSpec_1601496070418",
      "display_name": "Python 3.8.5 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "1f62fdb07a5c5211f1b5fcc7897261ef003ea6d0c918bffc758ffdf89d4e540f"
        }
      }
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.5-final",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "LoadAzureBlobParquet",
    "notebookId": 576599826755691
  },
  "nbformat": 4,
  "nbformat_minor": 0
}